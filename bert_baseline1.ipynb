{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8fghqT-Ukxv"
   },
   "outputs": [],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fj5ev1pTVn2Z"
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745289361880,
     "user": {
      "displayName": "Rex Chen",
      "userId": "03568709055767720474"
     },
     "user_tz": 300
    },
    "id": "rcyAKSViWb-Y"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745289789080,
     "user": {
      "displayName": "Rex Chen",
      "userId": "03568709055767720474"
     },
     "user_tz": 300
    },
    "id": "G08h8sA9Wj9w"
   },
   "outputs": [],
   "source": [
    "def connect_4o(input):\n",
    "  client = OpenAI(api_key=\"\")\n",
    "\n",
    "  response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"input_text\",\n",
    "            \"text\": \"Your task is to generate a clear, plain language answer for user inquiries about what factor contributes to the decision-making of the Autonomous Driving System. The focus is on what is the key feature making the system generate current decision.\\n \\nYou will receive a json file containing:\\n1. The identified key features with its description;\\n2. The numerical analysis value associated with identified key features. \\n3. The numerical value represents the velocity value of agent behavior.\\n4. The time interval between each scene time is 0.1s.\\n \\nBased on these inputs and information, generate a comprehensive answer that:\\n- Directly answer what feature results in the system behavior.\\n- Explains the result based on the calculated quantitative value.\\n- Uses non-technical language and avoids any mention of internal logic formulas or variable names.\\n \\nProvide your answer without any introductory or concluding remarks.\\n \\nExample: \\nInput: { \\\"key_feature\\\": \\\"observed position of agent_4 at scene time 20 at the 19 timestamp\\\",  \\\"quantitative_value\\\": -4.150000000000027}\\nAnswer: The reason why ego makes such decision at 2s is because it observes agent 4 decreases its speed 4.15m/s at 0.1s ago.\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"input_text\",\n",
    "            \"text\": input\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    text={\n",
    "      \"format\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    },\n",
    "    reasoning={},\n",
    "    tools=[],\n",
    "    temperature=1,\n",
    "    max_output_tokens=2048,\n",
    "    top_p=1,\n",
    "    store=True\n",
    "  )\n",
    "\n",
    "  gpt_response_message_content = next((out.content[0].text for out in response.output if out.type == 'message'), None)\n",
    "\n",
    "  return gpt_response_message_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745289789273,
     "user": {
      "displayName": "Rex Chen",
      "userId": "03568709055767720474"
     },
     "user_tz": 300
    },
    "id": "CfqAtHeZWrba"
   },
   "outputs": [],
   "source": [
    "def connect_gpt_o1(input):\n",
    "  client = OpenAI(api_key=\"\")\n",
    "\n",
    "  response = client.responses.create(model=\"o1\", input=[\n",
    "    {\n",
    "      \"role\": \"developer\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"input_text\",\n",
    "          \"text\": \"Your task is to generate a clear, plain language answer for user inquiries about what factor contributes to the decision-making of the Autonomous Driving System. The focus is on what is the key feature making the system generate current decision.\\n \\nYou will receive a json file containing:\\n1. The identified key features with its description;\\n2. The numerical analysis value associated with identified key features. \\n3. The numerical value represents the velocity value of agent behavior.\\n4. The time interval between each scene time is 0.1s.\\n \\nBased on these inputs and information, generate a comprehensive answer that:\\n- Directly answer what feature results in the system behavior.\\n- Explains the result based on the calculated quantitative value.\\n- Uses non-technical language and avoids any mention of internal logic formulas or variable names.\\n \\nProvide your answer without any introductory or concluding remarks.\\n \\nExample: \\nInput: { \\\"key_feature\\\": \\\"observed position of agent_4 at scene time 20 at the 19 timestamp\\\",  \\\"quantitative_value\\\": -4.150000000000027}\\nAnswer: The reason why ego makes such decision at 2s is because it observes agent 4 decreases its speed 4.15m/s at 0.1s ago.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"input_text\",\n",
    "          \"text\": input\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "    text={\n",
    "    \"format\": {\n",
    "      \"type\": \"text\"\n",
    "    }\n",
    "  },\n",
    "    reasoning={\n",
    "    \"effort\": \"low\"\n",
    "  },\n",
    "    tools=[],\n",
    "    store=True\n",
    "  )\n",
    "\n",
    "  gpt_response_message_content = next((out.content[0].text for out in response.output if out.type == 'message'), None)\n",
    "\n",
    "  return gpt_response_message_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745290192077,
     "user": {
      "displayName": "Rex Chen",
      "userId": "03568709055767720474"
     },
     "user_tz": 300
    },
    "id": "q8aeaGEAW9J0"
   },
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "import torch\n",
    "\n",
    "def calculate_bertscore(generated_texts, reference_texts, lang=\"en\", model_type=None, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Calculates the BERTScore (Precision, Recall, and F1) between generated and reference texts.\n",
    "\n",
    "    Args:\n",
    "        generated_texts (list of str): A list of generated sentences.\n",
    "        reference_texts (list of str or list of list of str): A list of reference sentences.\n",
    "                                                             If multiple references per generated text,\n",
    "                                                             provide a list of lists.\n",
    "        lang (str): The language of the texts (e.g., \"en\" for English).\n",
    "                    Required if model_type is not specified.\n",
    "        model_type (str, optional): The pre-trained BERT model to use (e.g., \"bert-base-uncased\").\n",
    "                                    If None, a default model for the specified language is used.\n",
    "        device (str, optional): The device to run the model on (\"cuda\" for GPU, \"cpu\" for CPU).\n",
    "                                Defaults to \"cuda:0\" if a GPU is available, otherwise \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three torch.Tensor objects:\n",
    "               - precision: BERTScore Precision for each sentence.\n",
    "               - recall: BERTScore Recall for each sentence.\n",
    "               - f1: BERTScore F1 score for each sentence.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        P, R, F1 = score(generated_texts, reference_texts, lang=lang, model_type=model_type, device=device)\n",
    "        return P, R, F1\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during BERTScore calculation: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745290532608,
     "user": {
      "displayName": "Rex Chen",
      "userId": "03568709055767720474"
     },
     "user_tz": 300
    },
    "id": "N1xLyYBXWwdu"
   },
   "outputs": [],
   "source": [
    "def evaluate_4o_assistant_accuracy(folder_path):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                dimension = data.get(\"dimension\")\n",
    "                key_feature = data.get(\"key_feature\")\n",
    "                extracted_info = data.get(\"extracted_info\")\n",
    "                quant_value = data.get(\"quantitative_value\")\n",
    "                ground_truth = data.get(\"ground_truth\")\n",
    "\n",
    "                query = key_feature + \" \" + str(quant_value)\n",
    "                ref = key_feature + \" \" + str(ground_truth)\n",
    "\n",
    "                if key_feature and extracted_info and ground_truth is not None:\n",
    "                    output_1 = connect_4o(query)\n",
    "                    output_2 = connect_4o(ref)\n",
    "                    # print(output_1, output_2)\n",
    "                    P, R, F1 = calculate_bertscore([output_1], [output_2])\n",
    "                    total += F1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "    if total == 0:\n",
    "        print(\"No valid files processed.\")\n",
    "        return 0.0\n",
    "    print(total)\n",
    "    accuracy = total/len(os.listdir(folder_path)) * 100\n",
    "    print(f\"Assistant Accuracy: {correct}/{total} = {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1745290533527,
     "user": {
      "displayName": "Rex Chen",
      "userId": "03568709055767720474"
     },
     "user_tz": 300
    },
    "id": "9N28WaeLakf6"
   },
   "outputs": [],
   "source": [
    "def evaluate_o1_assistant_accuracy(folder_path):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                dimension = data.get(\"dimension\")\n",
    "                key_feature = data.get(\"key_feature\")\n",
    "                extracted_info = data.get(\"extracted_info\")\n",
    "                quant_value = data.get(\"quantitative_value\")\n",
    "                ground_truth = data.get(\"ground_truth\")\n",
    "\n",
    "                query = key_feature + \" \" + str(quant_value)\n",
    "                ref = key_feature + \" \" + str(ground_truth)\n",
    "\n",
    "                if key_feature and extracted_info and ground_truth is not None:\n",
    "                    output_1 = connect_gpt_o1(query)\n",
    "                    output_2 = connect_gpt_o1(ref)\n",
    "                    # print(output_1, output_2)\n",
    "                    P, R, F1 = calculate_bertscore([output_1], [output_2])\n",
    "                    total += F1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "    if total == 0:\n",
    "        print(\"No valid files processed.\")\n",
    "        return 0.0\n",
    "\n",
    "    print(total)\n",
    "\n",
    "    accuracy = total/len(os.listdir(folder_path)) * 100\n",
    "    print(f\"Assistant Accuracy: {correct}/{total} = {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1001315,
     "status": "error",
     "timestamp": 1745291661322,
     "user": {
      "displayName": "Rex Chen",
      "userId": "03568709055767720474"
     },
     "user_tz": 300
    },
    "id": "9axi0ovmZqb_",
    "outputId": "9e19eea8-f039-43b8-fd26-e26912837f9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/23 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  4%|▍         | 1/23 [00:29<10:42, 29.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  9%|▊         | 2/23 [01:55<22:00, 62.89s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 13%|█▎        | 3/23 [02:41<18:25, 55.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 17%|█▋        | 4/23 [03:24<15:56, 50.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 22%|██▏       | 5/23 [03:59<13:23, 44.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 26%|██▌       | 6/23 [04:38<12:05, 42.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 30%|███       | 7/23 [05:30<12:16, 46.01s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 35%|███▍      | 8/23 [06:06<10:40, 42.70s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 39%|███▉      | 9/23 [06:52<10:10, 43.58s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 43%|████▎     | 10/23 [07:47<10:15, 47.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 48%|████▊     | 11/23 [08:22<08:41, 43.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 52%|█████▏    | 12/23 [08:56<07:26, 40.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 57%|█████▋    | 13/23 [09:34<06:38, 39.82s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 61%|██████    | 14/23 [10:39<07:05, 47.25s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 65%|██████▌   | 15/23 [11:15<05:52, 44.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 70%|██████▉   | 16/23 [12:08<05:26, 46.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 74%|███████▍  | 17/23 [13:00<04:49, 48.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 78%|███████▊  | 18/23 [13:35<03:41, 44.34s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 83%|████████▎ | 19/23 [14:29<03:08, 47.23s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 87%|████████▋ | 20/23 [14:58<02:05, 41.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 91%|█████████▏| 21/23 [15:30<01:17, 38.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 96%|█████████▌| 22/23 [16:07<00:38, 38.41s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 23/23 [16:41<00:00, 43.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21.9668])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2a76464568a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/output_jsons_after_logical_calculation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maccuracy_4o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_o1_assistant_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# accuracy_o3 = evaluate_o3_assistant_accuracy(folder_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The accuracy of quantitative value calculation after the logical reasoning is: {accuracy_4o:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e13912a85821>\u001b[0m in \u001b[0;36mevaluate_o1_assistant_accuracy\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Assistant Accuracy: {correct}/{total} = {accuracy:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_meta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    folder_path = \"/content/drive/MyDrive/output_jsons_after_logical_calculation\"\n",
    "    accuracy_4o = evaluate_o1_assistant_accuracy(folder_path)\n",
    "    # accuracy_o3 = evaluate_o3_assistant_accuracy(folder_path)\n",
    "    print(f\"The accuracy of quantitative value calculation after the logical reasoning is: {accuracy_4o:.2f}%\")\n",
    "    # print(f\"The accuracy of quantitative value calculation after the logical reasoning is: {accuracy_o3:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kn45Rri0ZvlV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOQsIqWbf30NCE0qqFKO844",
   "gpuType": "T4",
   "mount_file_id": "1ohtbpmLtfNo_iWeOKtnYcdVVnDZEYtQJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
